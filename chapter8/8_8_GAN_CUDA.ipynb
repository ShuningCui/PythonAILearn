{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, Normalize, Resize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = 'output'\n",
    "IMAGE_SIZE = 64    # 图像尺寸，原图是28*28的，缩放为64*64\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_CHANNEL = 1  # 输出图像通道数\n",
    "Z_DIM = 100\n",
    "G_HIDDEN = 64\n",
    "X_DIM = 64\n",
    "D_HIDDEN = 64\n",
    "EPOCH_NUM = 10\n",
    "REAL_LABEL = 1.0\n",
    "FAKE_LABEL = 0.0\n",
    "lr = 2e-4\n",
    "seed = np.random.randint(1, 10000)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor(0.5)\n",
    "std = torch.tensor(0.5)\n",
    "\n",
    "compose = Compose([Resize(IMAGE_SIZE, antialias=True) ,ToTensor(), Normalize(mean,std)])\n",
    "train_dataset = datasets.MNIST('./data', train=True, transform=compose, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"默认参数是按均匀分布随机初始化的\n",
    "       为了加速收敛，重新按正态分布初始化\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\" 合成网络将一个z_dim@1*1图像反向卷积为1@64*64的图像\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim=Z_DIM, g_hidden=G_HIDDEN, \n",
    "                 image_channel=IMAGE_CHANNEL) -> None:\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.g_hidden = g_hidden\n",
    "        self.image_channel = image_channel\n",
    "        self.cnn1 = nn.ConvTranspose2d(in_channels=self.z_dim, out_channels=\n",
    "            self.g_hidden*8, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=self.g_hidden*8)\n",
    "        self.cnn2 = nn.ConvTranspose2d(in_channels=self.g_hidden*8, out_channels=\n",
    "            self.g_hidden*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=self.g_hidden*4)\n",
    "        self.cnn3 = nn.ConvTranspose2d(in_channels=self.g_hidden*4, out_channels=\n",
    "            self.g_hidden*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=self.g_hidden*2)\n",
    "        self.cnn4 = nn.ConvTranspose2d(in_channels=self.g_hidden*2, out_channels=\n",
    "            self.g_hidden, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=self.g_hidden)\n",
    "        self.cnn5 = nn.ConvTranspose2d(in_channels=self.g_hidden, out_channels=\n",
    "            self.image_channel,kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # 输入：100@1*1\n",
    "        X = self.cnn1(X)\n",
    "        X = self.bn1(X)\n",
    "        X = F.relu(X, inplace=True)\n",
    "        # 输入：512@4*4\n",
    "        X = self.cnn2(X)\n",
    "        X = self.bn2(X)\n",
    "        X = F.relu(X, inplace=True)\n",
    "        # 输入：256@8*8\n",
    "        X = self.cnn3(X)\n",
    "        X = self.bn3(X)\n",
    "        X = F.relu(X, inplace=True)\n",
    "        # 输入：128@16*16\n",
    "        X = self.cnn4(X)\n",
    "        X = self.bn4(X)\n",
    "        X = F.relu(X, inplace=True)\n",
    "        # 输入：64@32*32\n",
    "        X = self.cnn5(X)\n",
    "        X = F.tanh(X)\n",
    "        # 输出：1@64*64\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" 鉴别网络是一个分类网络，但是没有线性层\n",
    "        通过卷积将输入1@64*64变换为1@1*1\n",
    "    \"\"\"\n",
    "    def __init__(self, d_hidden=D_HIDDEN, image_channel=IMAGE_CHANNEL) -> None:\n",
    "        super().__init__()\n",
    "        self.image_channel = image_channel\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels=self.image_channel, out_channels=\n",
    "            self.d_hidden, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.cnn2 = nn.Conv2d(in_channels=self.d_hidden, out_channels=\n",
    "            self.d_hidden*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=self.d_hidden*2)\n",
    "        self.cnn3 = nn.Conv2d(in_channels=self.d_hidden*2, out_channels=\n",
    "            self.d_hidden*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=self.d_hidden*4)\n",
    "        self.cnn4 = nn.Conv2d(in_channels=self.d_hidden*4, out_channels=\n",
    "            self.d_hidden*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=self.d_hidden*8)\n",
    "        self.cnn5 = nn.Conv2d(in_channels=self.d_hidden*8, out_channels=1,\n",
    "            kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # 1@64*64\n",
    "        X = self.cnn1(X)\n",
    "        X = F.leaky_relu(X, 0.2, inplace=True)\n",
    "        # 64@32*32\n",
    "        X = self.cnn2(X)\n",
    "        X = self.bn2(X)\n",
    "        X = F.leaky_relu(X, 0.2, inplace=True)\n",
    "        # 128@16*16\n",
    "        X = self.cnn3(X)\n",
    "        X = self.bn3(X)\n",
    "        X = F.leaky_relu(X, 0.2, inplace=True)\n",
    "        # 256@8*8\n",
    "        X = self.cnn4(X)\n",
    "        X = self.bn4(X)\n",
    "        X = F.leaky_relu(X, 0.2, inplace=True)\n",
    "        # 512@4*4\n",
    "        X = self.cnn5(X)\n",
    "        # 1@1*1\n",
    "        X = F.sigmoid(X)\n",
    "        return X.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [0/3750] loss_D_real: 1.2139                   loss_D_fake: 0.5433 loss_G: 3.8178\n",
      "Epoch 0 [100/3750] loss_D_real: 0.0047                   loss_D_fake: 0.0147 loss_G: 8.1145\n",
      "Epoch 0 [200/3750] loss_D_real: 0.0002                   loss_D_fake: 0.0029 loss_G: 7.1767\n",
      "Epoch 0 [300/3750] loss_D_real: 0.0001                   loss_D_fake: 0.0022 loss_G: 7.5539\n",
      "Epoch 0 [400/3750] loss_D_real: 0.0010                   loss_D_fake: 0.0029 loss_G: 7.8670\n",
      "Epoch 0 [500/3750] loss_D_real: 5.5719                   loss_D_fake: 0.0000 loss_G: 10.0703\n",
      "Epoch 0 [600/3750] loss_D_real: 0.2168                   loss_D_fake: 0.0020 loss_G: 11.1592\n",
      "Epoch 0 [700/3750] loss_D_real: 0.0448                   loss_D_fake: 0.0540 loss_G: 4.7949\n",
      "Epoch 0 [800/3750] loss_D_real: 0.0143                   loss_D_fake: 0.2165 loss_G: 5.5906\n",
      "Epoch 0 [900/3750] loss_D_real: 0.0147                   loss_D_fake: 0.0744 loss_G: 6.1578\n",
      "Epoch 0 [1000/3750] loss_D_real: 0.0128                   loss_D_fake: 0.1066 loss_G: 3.5338\n",
      "Epoch 0 [1100/3750] loss_D_real: 0.0126                   loss_D_fake: 0.0154 loss_G: 5.0268\n",
      "Epoch 0 [1200/3750] loss_D_real: 0.0265                   loss_D_fake: 0.5490 loss_G: 6.4003\n",
      "Epoch 0 [1300/3750] loss_D_real: 0.1340                   loss_D_fake: 0.2953 loss_G: 2.9731\n",
      "Epoch 0 [1400/3750] loss_D_real: 0.0646                   loss_D_fake: 0.0265 loss_G: 4.7177\n",
      "Epoch 0 [1500/3750] loss_D_real: 0.0702                   loss_D_fake: 0.0921 loss_G: 3.2026\n",
      "Epoch 0 [1600/3750] loss_D_real: 0.0127                   loss_D_fake: 0.3659 loss_G: 4.9875\n",
      "Epoch 0 [1700/3750] loss_D_real: 0.2187                   loss_D_fake: 0.0197 loss_G: 2.7234\n",
      "Epoch 0 [1800/3750] loss_D_real: 0.0741                   loss_D_fake: 0.0342 loss_G: 3.0168\n",
      "Epoch 0 [1900/3750] loss_D_real: 0.1152                   loss_D_fake: 0.0624 loss_G: 3.3702\n",
      "Epoch 0 [2000/3750] loss_D_real: 0.5665                   loss_D_fake: 0.0771 loss_G: 0.9236\n",
      "Epoch 0 [2100/3750] loss_D_real: 0.0431                   loss_D_fake: 0.0852 loss_G: 3.4855\n",
      "Epoch 0 [2200/3750] loss_D_real: 0.2713                   loss_D_fake: 0.0412 loss_G: 2.5950\n",
      "Epoch 0 [2300/3750] loss_D_real: 0.0016                   loss_D_fake: 2.3599 loss_G: 7.0670\n",
      "Epoch 0 [2400/3750] loss_D_real: 0.0137                   loss_D_fake: 0.7549 loss_G: 5.6423\n",
      "Epoch 0 [2500/3750] loss_D_real: 0.0084                   loss_D_fake: 0.7027 loss_G: 7.0826\n",
      "Epoch 0 [2600/3750] loss_D_real: 0.5553                   loss_D_fake: 0.0275 loss_G: 2.6289\n",
      "Epoch 0 [2700/3750] loss_D_real: 0.3062                   loss_D_fake: 0.0457 loss_G: 2.1141\n",
      "Epoch 0 [2800/3750] loss_D_real: 0.1635                   loss_D_fake: 0.1223 loss_G: 4.4064\n",
      "Epoch 0 [2900/3750] loss_D_real: 0.0885                   loss_D_fake: 0.0127 loss_G: 4.2829\n",
      "Epoch 0 [3000/3750] loss_D_real: 0.2385                   loss_D_fake: 0.0609 loss_G: 2.3911\n",
      "Epoch 0 [3100/3750] loss_D_real: 0.0289                   loss_D_fake: 0.0615 loss_G: 4.0115\n",
      "Epoch 0 [3200/3750] loss_D_real: 0.0893                   loss_D_fake: 0.0043 loss_G: 5.0943\n",
      "Epoch 0 [3300/3750] loss_D_real: 0.0418                   loss_D_fake: 0.0024 loss_G: 6.4501\n",
      "Epoch 0 [3400/3750] loss_D_real: 0.7901                   loss_D_fake: 0.0164 loss_G: 4.8867\n",
      "Epoch 0 [3500/3750] loss_D_real: 0.0800                   loss_D_fake: 0.0042 loss_G: 5.2831\n",
      "Epoch 0 [3600/3750] loss_D_real: 1.6175                   loss_D_fake: 0.0034 loss_G: 3.1357\n",
      "Epoch 0 [3700/3750] loss_D_real: 0.0146                   loss_D_fake: 0.0151 loss_G: 5.0734\n",
      "Epoch 1 [0/3750] loss_D_real: 0.0938                   loss_D_fake: 1.2722 loss_G: 3.6189\n",
      "Epoch 1 [100/3750] loss_D_real: 0.0062                   loss_D_fake: 0.7097 loss_G: 4.9826\n",
      "Epoch 1 [200/3750] loss_D_real: 0.0720                   loss_D_fake: 0.1064 loss_G: 4.6399\n",
      "Epoch 1 [300/3750] loss_D_real: 0.2257                   loss_D_fake: 0.2008 loss_G: 3.6589\n",
      "Epoch 1 [400/3750] loss_D_real: 0.2386                   loss_D_fake: 0.0845 loss_G: 1.9754\n",
      "Epoch 1 [500/3750] loss_D_real: 0.0407                   loss_D_fake: 0.0443 loss_G: 3.9472\n",
      "Epoch 1 [600/3750] loss_D_real: 0.2338                   loss_D_fake: 0.0256 loss_G: 2.8783\n",
      "Epoch 1 [700/3750] loss_D_real: 0.0220                   loss_D_fake: 0.0615 loss_G: 5.2863\n",
      "Epoch 1 [800/3750] loss_D_real: 0.0021                   loss_D_fake: 0.0622 loss_G: 5.9630\n",
      "Epoch 1 [900/3750] loss_D_real: 0.0254                   loss_D_fake: 0.0453 loss_G: 4.4230\n",
      "Epoch 1 [1000/3750] loss_D_real: 0.1084                   loss_D_fake: 0.0078 loss_G: 3.5016\n",
      "Epoch 1 [1100/3750] loss_D_real: 0.2227                   loss_D_fake: 2.1889 loss_G: 3.6340\n",
      "Epoch 1 [1200/3750] loss_D_real: 0.0217                   loss_D_fake: 0.0412 loss_G: 3.4009\n",
      "Epoch 1 [1300/3750] loss_D_real: 0.0113                   loss_D_fake: 0.0059 loss_G: 5.8033\n",
      "Epoch 1 [1400/3750] loss_D_real: 0.0062                   loss_D_fake: 0.2938 loss_G: 5.5932\n",
      "Epoch 1 [1500/3750] loss_D_real: 0.0139                   loss_D_fake: 0.0342 loss_G: 6.0128\n",
      "Epoch 1 [1600/3750] loss_D_real: 0.2599                   loss_D_fake: 0.0384 loss_G: 2.9641\n",
      "Epoch 1 [1700/3750] loss_D_real: 0.0158                   loss_D_fake: 0.0269 loss_G: 5.1618\n",
      "Epoch 1 [1800/3750] loss_D_real: 0.0020                   loss_D_fake: 0.0528 loss_G: 4.2619\n",
      "Epoch 1 [1900/3750] loss_D_real: 0.2516                   loss_D_fake: 0.0115 loss_G: 4.4882\n",
      "Epoch 1 [2000/3750] loss_D_real: 0.0467                   loss_D_fake: 0.0287 loss_G: 4.8169\n",
      "Epoch 1 [2100/3750] loss_D_real: 0.0709                   loss_D_fake: 0.1541 loss_G: 4.2630\n",
      "Epoch 1 [2200/3750] loss_D_real: 0.0321                   loss_D_fake: 0.0751 loss_G: 4.0421\n",
      "Epoch 1 [2300/3750] loss_D_real: 0.1059                   loss_D_fake: 0.2135 loss_G: 5.0046\n",
      "Epoch 1 [2400/3750] loss_D_real: 0.3076                   loss_D_fake: 0.3152 loss_G: 3.7119\n",
      "Epoch 1 [2500/3750] loss_D_real: 0.1775                   loss_D_fake: 0.0636 loss_G: 2.8621\n",
      "Epoch 1 [2600/3750] loss_D_real: 0.0057                   loss_D_fake: 0.0171 loss_G: 6.0469\n",
      "Epoch 1 [2700/3750] loss_D_real: 0.0036                   loss_D_fake: 0.0150 loss_G: 4.8811\n",
      "Epoch 1 [2800/3750] loss_D_real: 0.1012                   loss_D_fake: 0.0153 loss_G: 4.1706\n",
      "Epoch 1 [2900/3750] loss_D_real: 0.0040                   loss_D_fake: 0.0201 loss_G: 4.7756\n",
      "Epoch 1 [3000/3750] loss_D_real: 0.0484                   loss_D_fake: 0.1957 loss_G: 2.1393\n",
      "Epoch 1 [3100/3750] loss_D_real: 0.0266                   loss_D_fake: 0.0865 loss_G: 5.8366\n",
      "Epoch 1 [3200/3750] loss_D_real: 0.0039                   loss_D_fake: 0.0669 loss_G: 4.2019\n",
      "Epoch 1 [3300/3750] loss_D_real: 0.0913                   loss_D_fake: 0.0319 loss_G: 5.0623\n",
      "Epoch 1 [3400/3750] loss_D_real: 0.0724                   loss_D_fake: 0.2360 loss_G: 3.8441\n",
      "Epoch 1 [3500/3750] loss_D_real: 0.0031                   loss_D_fake: 0.0505 loss_G: 4.7317\n",
      "Epoch 1 [3600/3750] loss_D_real: 0.3001                   loss_D_fake: 0.0116 loss_G: 2.2980\n",
      "Epoch 1 [3700/3750] loss_D_real: 0.0040                   loss_D_fake: 0.1957 loss_G: 7.4095\n",
      "Epoch 2 [0/3750] loss_D_real: 0.1255                   loss_D_fake: 0.1637 loss_G: 4.7756\n",
      "Epoch 2 [100/3750] loss_D_real: 0.0799                   loss_D_fake: 0.1597 loss_G: 4.6661\n",
      "Epoch 2 [200/3750] loss_D_real: 0.0066                   loss_D_fake: 0.0403 loss_G: 4.3141\n",
      "Epoch 2 [300/3750] loss_D_real: 0.0243                   loss_D_fake: 0.0013 loss_G: 7.3932\n",
      "Epoch 2 [400/3750] loss_D_real: 0.0002                   loss_D_fake: 0.1213 loss_G: 3.6150\n",
      "Epoch 2 [500/3750] loss_D_real: 0.0193                   loss_D_fake: 0.0018 loss_G: 6.7624\n",
      "Epoch 2 [600/3750] loss_D_real: 0.1588                   loss_D_fake: 0.0628 loss_G: 4.2729\n",
      "Epoch 2 [700/3750] loss_D_real: 0.2178                   loss_D_fake: 0.0399 loss_G: 3.9038\n",
      "Epoch 2 [800/3750] loss_D_real: 0.0793                   loss_D_fake: 0.0035 loss_G: 4.5165\n",
      "Epoch 2 [900/3750] loss_D_real: 0.0449                   loss_D_fake: 0.5139 loss_G: 6.4166\n",
      "Epoch 2 [1000/3750] loss_D_real: 2.5159                   loss_D_fake: 0.0022 loss_G: 2.0959\n",
      "Epoch 2 [1100/3750] loss_D_real: 0.0058                   loss_D_fake: 0.0414 loss_G: 5.3601\n",
      "Epoch 2 [1200/3750] loss_D_real: 0.0212                   loss_D_fake: 0.0421 loss_G: 4.6548\n",
      "Epoch 2 [1300/3750] loss_D_real: 0.2174                   loss_D_fake: 0.0061 loss_G: 3.8044\n",
      "Epoch 2 [1400/3750] loss_D_real: 0.0028                   loss_D_fake: 0.0033 loss_G: 7.0295\n",
      "Epoch 2 [1500/3750] loss_D_real: 0.0144                   loss_D_fake: 0.0013 loss_G: 8.2464\n",
      "Epoch 2 [1600/3750] loss_D_real: 0.0014                   loss_D_fake: 0.0021 loss_G: 6.6141\n",
      "Epoch 2 [1700/3750] loss_D_real: 0.9170                   loss_D_fake: 0.0452 loss_G: 2.3454\n",
      "Epoch 2 [1800/3750] loss_D_real: 0.0758                   loss_D_fake: 0.0365 loss_G: 4.9489\n",
      "Epoch 2 [1900/3750] loss_D_real: 0.0070                   loss_D_fake: 0.0960 loss_G: 4.6699\n",
      "Epoch 2 [2000/3750] loss_D_real: 0.0875                   loss_D_fake: 0.0962 loss_G: 3.8231\n",
      "Epoch 2 [2100/3750] loss_D_real: 0.2186                   loss_D_fake: 0.0022 loss_G: 4.1250\n",
      "Epoch 2 [2200/3750] loss_D_real: 0.0050                   loss_D_fake: 0.0993 loss_G: 4.7903\n",
      "Epoch 2 [2300/3750] loss_D_real: 0.0222                   loss_D_fake: 0.1983 loss_G: 5.7973\n",
      "Epoch 2 [2400/3750] loss_D_real: 0.0600                   loss_D_fake: 0.0267 loss_G: 4.2777\n",
      "Epoch 2 [2500/3750] loss_D_real: 0.5102                   loss_D_fake: 0.0271 loss_G: 2.4701\n",
      "Epoch 2 [2600/3750] loss_D_real: 0.0413                   loss_D_fake: 0.0331 loss_G: 4.3561\n",
      "Epoch 2 [2700/3750] loss_D_real: 0.0931                   loss_D_fake: 0.0130 loss_G: 4.5902\n",
      "Epoch 2 [2800/3750] loss_D_real: 0.0179                   loss_D_fake: 0.0030 loss_G: 6.2318\n",
      "Epoch 2 [2900/3750] loss_D_real: 0.0123                   loss_D_fake: 0.0036 loss_G: 5.7886\n",
      "Epoch 2 [3000/3750] loss_D_real: 0.0015                   loss_D_fake: 0.0042 loss_G: 5.5747\n",
      "Epoch 2 [3100/3750] loss_D_real: 0.0330                   loss_D_fake: 0.0031 loss_G: 6.3864\n",
      "Epoch 2 [3200/3750] loss_D_real: 0.0634                   loss_D_fake: 0.0609 loss_G: 4.0157\n",
      "Epoch 2 [3300/3750] loss_D_real: 0.0864                   loss_D_fake: 0.0085 loss_G: 3.8925\n",
      "Epoch 2 [3400/3750] loss_D_real: 0.0015                   loss_D_fake: 0.0402 loss_G: 4.2368\n",
      "Epoch 2 [3500/3750] loss_D_real: 0.0493                   loss_D_fake: 0.0022 loss_G: 6.1069\n",
      "Epoch 2 [3600/3750] loss_D_real: 0.0017                   loss_D_fake: 0.3879 loss_G: 8.4799\n",
      "Epoch 2 [3700/3750] loss_D_real: 0.0181                   loss_D_fake: 0.0015 loss_G: 6.8095\n",
      "Epoch 3 [0/3750] loss_D_real: 0.0575                   loss_D_fake: 0.0039 loss_G: 5.4094\n",
      "Epoch 3 [100/3750] loss_D_real: 0.0078                   loss_D_fake: 1.4113 loss_G: 10.6962\n",
      "Epoch 3 [200/3750] loss_D_real: 0.0025                   loss_D_fake: 1.2600 loss_G: 5.9253\n",
      "Epoch 3 [300/3750] loss_D_real: 0.0034                   loss_D_fake: 0.0136 loss_G: 5.0747\n",
      "Epoch 3 [400/3750] loss_D_real: 0.0377                   loss_D_fake: 0.0041 loss_G: 4.5534\n",
      "Epoch 3 [500/3750] loss_D_real: 0.7258                   loss_D_fake: 0.1255 loss_G: 2.5120\n",
      "Epoch 3 [600/3750] loss_D_real: 4.4935                   loss_D_fake: 0.0001 loss_G: 2.3499\n",
      "Epoch 3 [700/3750] loss_D_real: 0.1158                   loss_D_fake: 0.0048 loss_G: 5.7406\n",
      "Epoch 3 [800/3750] loss_D_real: 0.0059                   loss_D_fake: 0.0098 loss_G: 5.2144\n",
      "Epoch 3 [900/3750] loss_D_real: 0.0008                   loss_D_fake: 0.1348 loss_G: 5.0117\n",
      "Epoch 3 [1000/3750] loss_D_real: 0.0190                   loss_D_fake: 0.0046 loss_G: 5.4473\n",
      "Epoch 3 [1100/3750] loss_D_real: 0.0210                   loss_D_fake: 0.0241 loss_G: 3.9098\n",
      "Epoch 3 [1200/3750] loss_D_real: 0.0116                   loss_D_fake: 0.0054 loss_G: 5.9011\n",
      "Epoch 3 [1300/3750] loss_D_real: 0.0004                   loss_D_fake: 0.4083 loss_G: 12.8736\n",
      "Epoch 3 [1400/3750] loss_D_real: 0.0243                   loss_D_fake: 0.0050 loss_G: 6.8431\n",
      "Epoch 3 [1500/3750] loss_D_real: 0.0028                   loss_D_fake: 0.0759 loss_G: 5.1833\n",
      "Epoch 3 [1600/3750] loss_D_real: 0.0073                   loss_D_fake: 0.0066 loss_G: 6.6627\n",
      "Epoch 3 [1700/3750] loss_D_real: 0.0001                   loss_D_fake: 0.0490 loss_G: 6.4952\n",
      "Epoch 3 [1800/3750] loss_D_real: 0.0112                   loss_D_fake: 0.0073 loss_G: 6.4058\n",
      "Epoch 3 [1900/3750] loss_D_real: 0.1376                   loss_D_fake: 0.0039 loss_G: 5.5582\n",
      "Epoch 3 [2000/3750] loss_D_real: 0.0474                   loss_D_fake: 0.0769 loss_G: 4.2192\n",
      "Epoch 3 [2100/3750] loss_D_real: 0.7499                   loss_D_fake: 0.0212 loss_G: 3.5477\n",
      "Epoch 3 [2200/3750] loss_D_real: 0.0015                   loss_D_fake: 0.0262 loss_G: 5.1548\n",
      "Epoch 3 [2300/3750] loss_D_real: 0.0015                   loss_D_fake: 0.0098 loss_G: 7.3937\n",
      "Epoch 3 [2400/3750] loss_D_real: 0.0100                   loss_D_fake: 0.0346 loss_G: 4.6538\n",
      "Epoch 3 [2500/3750] loss_D_real: 0.0083                   loss_D_fake: 0.0052 loss_G: 5.7012\n",
      "Epoch 3 [2600/3750] loss_D_real: 0.0012                   loss_D_fake: 0.0014 loss_G: 7.1114\n",
      "Epoch 3 [2700/3750] loss_D_real: 0.0753                   loss_D_fake: 0.0773 loss_G: 2.8715\n",
      "Epoch 3 [2800/3750] loss_D_real: 0.2268                   loss_D_fake: 0.0032 loss_G: 6.8823\n",
      "Epoch 3 [2900/3750] loss_D_real: 3.2212                   loss_D_fake: 0.0013 loss_G: 1.1386\n",
      "Epoch 3 [3000/3750] loss_D_real: 0.1922                   loss_D_fake: 0.0198 loss_G: 4.0521\n",
      "Epoch 3 [3100/3750] loss_D_real: 0.0020                   loss_D_fake: 1.6282 loss_G: 8.6005\n",
      "Epoch 3 [3200/3750] loss_D_real: 0.0139                   loss_D_fake: 0.0068 loss_G: 5.7725\n",
      "Epoch 3 [3300/3750] loss_D_real: 0.0731                   loss_D_fake: 0.2203 loss_G: 2.8761\n",
      "Epoch 3 [3400/3750] loss_D_real: 0.0143                   loss_D_fake: 0.0097 loss_G: 3.5111\n",
      "Epoch 3 [3500/3750] loss_D_real: 0.0187                   loss_D_fake: 0.0074 loss_G: 5.6513\n",
      "Epoch 3 [3600/3750] loss_D_real: 0.0015                   loss_D_fake: 0.0016 loss_G: 7.3934\n",
      "Epoch 3 [3700/3750] loss_D_real: 0.0266                   loss_D_fake: 0.0008 loss_G: 6.9942\n",
      "Epoch 4 [0/3750] loss_D_real: 0.0042                   loss_D_fake: 0.0033 loss_G: 6.2178\n",
      "Epoch 4 [100/3750] loss_D_real: 0.0197                   loss_D_fake: 0.0530 loss_G: 5.2841\n",
      "Epoch 4 [200/3750] loss_D_real: 0.1092                   loss_D_fake: 0.3315 loss_G: 3.9620\n",
      "Epoch 4 [300/3750] loss_D_real: 0.0660                   loss_D_fake: 0.1807 loss_G: 3.1939\n",
      "Epoch 4 [400/3750] loss_D_real: 0.0004                   loss_D_fake: 0.0068 loss_G: 5.7435\n",
      "Epoch 4 [500/3750] loss_D_real: 0.0103                   loss_D_fake: 0.0154 loss_G: 4.8531\n",
      "Epoch 4 [600/3750] loss_D_real: 0.0347                   loss_D_fake: 0.2031 loss_G: 4.5540\n",
      "Epoch 4 [700/3750] loss_D_real: 0.0077                   loss_D_fake: 0.0090 loss_G: 5.4754\n",
      "Epoch 4 [800/3750] loss_D_real: 0.0805                   loss_D_fake: 0.3234 loss_G: 3.8255\n",
      "Epoch 4 [900/3750] loss_D_real: 0.0014                   loss_D_fake: 0.2339 loss_G: 7.5790\n",
      "Epoch 4 [1000/3750] loss_D_real: 0.0118                   loss_D_fake: 0.0148 loss_G: 6.1985\n",
      "Epoch 4 [1100/3750] loss_D_real: 0.0004                   loss_D_fake: 0.0029 loss_G: 6.8912\n",
      "Epoch 4 [1200/3750] loss_D_real: 0.0075                   loss_D_fake: 0.0010 loss_G: 7.1623\n",
      "Epoch 4 [1300/3750] loss_D_real: 0.3008                   loss_D_fake: 0.0002 loss_G: 6.5789\n",
      "Epoch 4 [1400/3750] loss_D_real: 0.0012                   loss_D_fake: 0.2170 loss_G: 5.1633\n",
      "Epoch 4 [1500/3750] loss_D_real: 0.3476                   loss_D_fake: 0.0003 loss_G: 6.9436\n",
      "Epoch 4 [1600/3750] loss_D_real: 0.0063                   loss_D_fake: 0.0222 loss_G: 5.1333\n",
      "Epoch 4 [1700/3750] loss_D_real: 0.0133                   loss_D_fake: 0.0896 loss_G: 7.6459\n",
      "Epoch 4 [1800/3750] loss_D_real: 0.0014                   loss_D_fake: 0.0151 loss_G: 6.1736\n",
      "Epoch 4 [1900/3750] loss_D_real: 0.0166                   loss_D_fake: 0.0032 loss_G: 6.0769\n",
      "Epoch 4 [2000/3750] loss_D_real: 0.0073                   loss_D_fake: 0.0547 loss_G: 3.5142\n",
      "Epoch 4 [2100/3750] loss_D_real: 0.0010                   loss_D_fake: 0.2763 loss_G: 9.7645\n",
      "Epoch 4 [2200/3750] loss_D_real: 0.0481                   loss_D_fake: 0.0408 loss_G: 4.3664\n",
      "Epoch 4 [2300/3750] loss_D_real: 0.0594                   loss_D_fake: 0.0186 loss_G: 4.5824\n",
      "Epoch 4 [2400/3750] loss_D_real: 0.0047                   loss_D_fake: 0.1291 loss_G: 5.7351\n",
      "Epoch 4 [2500/3750] loss_D_real: 0.0139                   loss_D_fake: 0.0083 loss_G: 4.8997\n",
      "Epoch 4 [2600/3750] loss_D_real: 0.0678                   loss_D_fake: 0.0009 loss_G: 7.1377\n",
      "Epoch 4 [2700/3750] loss_D_real: 0.0035                   loss_D_fake: 0.0029 loss_G: 6.7780\n",
      "Epoch 4 [2800/3750] loss_D_real: 0.0123                   loss_D_fake: 0.0033 loss_G: 6.1667\n",
      "Epoch 4 [2900/3750] loss_D_real: 0.0019                   loss_D_fake: 0.5505 loss_G: 2.8536\n",
      "Epoch 4 [3000/3750] loss_D_real: 0.0015                   loss_D_fake: 0.0295 loss_G: 6.0112\n",
      "Epoch 4 [3100/3750] loss_D_real: 0.0403                   loss_D_fake: 0.7918 loss_G: 4.9711\n",
      "Epoch 4 [3200/3750] loss_D_real: 0.0095                   loss_D_fake: 0.6483 loss_G: 7.6784\n",
      "Epoch 4 [3300/3750] loss_D_real: 0.0273                   loss_D_fake: 0.0195 loss_G: 5.4686\n",
      "Epoch 4 [3400/3750] loss_D_real: 0.0706                   loss_D_fake: 0.0061 loss_G: 4.4662\n",
      "Epoch 4 [3500/3750] loss_D_real: 0.0804                   loss_D_fake: 0.0002 loss_G: 6.0476\n",
      "Epoch 4 [3600/3750] loss_D_real: 0.0003                   loss_D_fake: 0.0086 loss_G: 6.0335\n",
      "Epoch 4 [3700/3750] loss_D_real: 0.0085                   loss_D_fake: 0.1356 loss_G: 5.8020\n",
      "Epoch 5 [0/3750] loss_D_real: 0.0014                   loss_D_fake: 0.0015 loss_G: 7.3364\n",
      "Epoch 5 [100/3750] loss_D_real: 0.0016                   loss_D_fake: 0.0055 loss_G: 5.9489\n",
      "Epoch 5 [200/3750] loss_D_real: 0.2499                   loss_D_fake: 0.5124 loss_G: 4.4439\n",
      "Epoch 5 [300/3750] loss_D_real: 0.0351                   loss_D_fake: 0.0015 loss_G: 7.1776\n",
      "Epoch 5 [400/3750] loss_D_real: 0.6034                   loss_D_fake: 0.0193 loss_G: 3.1011\n",
      "Epoch 5 [500/3750] loss_D_real: 0.0336                   loss_D_fake: 0.4709 loss_G: 5.6177\n",
      "Epoch 5 [600/3750] loss_D_real: 0.0073                   loss_D_fake: 0.0027 loss_G: 7.1957\n",
      "Epoch 5 [700/3750] loss_D_real: 0.0031                   loss_D_fake: 0.0107 loss_G: 5.5355\n",
      "Epoch 5 [800/3750] loss_D_real: 0.3234                   loss_D_fake: 0.0393 loss_G: 2.5307\n",
      "Epoch 5 [900/3750] loss_D_real: 0.0030                   loss_D_fake: 0.2305 loss_G: 9.4289\n",
      "Epoch 5 [1000/3750] loss_D_real: 0.0017                   loss_D_fake: 0.6018 loss_G: 8.8583\n",
      "Epoch 5 [1100/3750] loss_D_real: 0.0038                   loss_D_fake: 0.1255 loss_G: 6.5552\n",
      "Epoch 5 [1200/3750] loss_D_real: 0.0443                   loss_D_fake: 0.0009 loss_G: 6.7520\n",
      "Epoch 5 [1300/3750] loss_D_real: 0.0021                   loss_D_fake: 0.0056 loss_G: 6.4235\n",
      "Epoch 5 [1400/3750] loss_D_real: 0.0016                   loss_D_fake: 0.0257 loss_G: 5.5534\n",
      "Epoch 5 [1500/3750] loss_D_real: 0.0439                   loss_D_fake: 0.1173 loss_G: 4.7171\n",
      "Epoch 5 [1600/3750] loss_D_real: 0.0034                   loss_D_fake: 0.0005 loss_G: 9.7052\n",
      "Epoch 5 [1700/3750] loss_D_real: 0.4474                   loss_D_fake: 0.2837 loss_G: 2.0211\n",
      "Epoch 5 [1800/3750] loss_D_real: 0.0014                   loss_D_fake: 0.0041 loss_G: 6.1053\n",
      "Epoch 5 [1900/3750] loss_D_real: 0.0037                   loss_D_fake: 0.0011 loss_G: 7.2860\n",
      "Epoch 5 [2000/3750] loss_D_real: 0.0228                   loss_D_fake: 0.0098 loss_G: 5.6070\n",
      "Epoch 5 [2100/3750] loss_D_real: 0.0165                   loss_D_fake: 0.1134 loss_G: 4.8133\n",
      "Epoch 5 [2200/3750] loss_D_real: 0.6073                   loss_D_fake: 0.0009 loss_G: 1.5444\n",
      "Epoch 5 [2300/3750] loss_D_real: 0.0052                   loss_D_fake: 0.0173 loss_G: 5.3631\n",
      "Epoch 5 [2400/3750] loss_D_real: 0.0062                   loss_D_fake: 0.0061 loss_G: 5.8287\n",
      "Epoch 5 [2500/3750] loss_D_real: 0.0036                   loss_D_fake: 0.0038 loss_G: 6.0945\n",
      "Epoch 5 [2600/3750] loss_D_real: 0.0050                   loss_D_fake: 0.0361 loss_G: 4.7043\n",
      "Epoch 5 [2700/3750] loss_D_real: 0.0011                   loss_D_fake: 0.0009 loss_G: 7.7444\n",
      "Epoch 5 [2800/3750] loss_D_real: 0.0002                   loss_D_fake: 0.0010 loss_G: 7.3619\n",
      "Epoch 5 [2900/3750] loss_D_real: 0.0013                   loss_D_fake: 0.0033 loss_G: 6.3051\n",
      "Epoch 5 [3000/3750] loss_D_real: 0.0030                   loss_D_fake: 0.0021 loss_G: 7.3982\n",
      "Epoch 5 [3100/3750] loss_D_real: 0.0451                   loss_D_fake: 0.0299 loss_G: 3.5201\n",
      "Epoch 5 [3200/3750] loss_D_real: 0.0128                   loss_D_fake: 0.0025 loss_G: 6.6937\n",
      "Epoch 5 [3300/3750] loss_D_real: 0.0000                   loss_D_fake: 1.3826 loss_G: 30.9688\n",
      "Epoch 5 [3400/3750] loss_D_real: 0.0032                   loss_D_fake: 0.0185 loss_G: 4.8298\n",
      "Epoch 5 [3500/3750] loss_D_real: 0.0111                   loss_D_fake: 0.0106 loss_G: 5.6090\n",
      "Epoch 5 [3600/3750] loss_D_real: 0.1195                   loss_D_fake: 0.0048 loss_G: 4.3117\n",
      "Epoch 5 [3700/3750] loss_D_real: 0.0086                   loss_D_fake: 0.0592 loss_G: 6.4652\n",
      "Epoch 6 [0/3750] loss_D_real: 0.0052                   loss_D_fake: 0.2096 loss_G: 3.8344\n",
      "Epoch 6 [100/3750] loss_D_real: 0.0057                   loss_D_fake: 0.2296 loss_G: 6.2719\n",
      "Epoch 6 [200/3750] loss_D_real: 0.6519                   loss_D_fake: 0.0005 loss_G: 3.0004\n",
      "Epoch 6 [300/3750] loss_D_real: 0.0010                   loss_D_fake: 0.0165 loss_G: 5.7127\n",
      "Epoch 6 [400/3750] loss_D_real: 0.0033                   loss_D_fake: 0.0009 loss_G: 7.5009\n",
      "Epoch 6 [500/3750] loss_D_real: 0.0027                   loss_D_fake: 0.0111 loss_G: 6.8652\n",
      "Epoch 6 [600/3750] loss_D_real: 0.0016                   loss_D_fake: 0.0004 loss_G: 8.1643\n",
      "Epoch 6 [700/3750] loss_D_real: 0.2034                   loss_D_fake: 0.0358 loss_G: 4.1219\n",
      "Epoch 6 [800/3750] loss_D_real: 0.5984                   loss_D_fake: 0.0001 loss_G: 5.1114\n",
      "Epoch 6 [900/3750] loss_D_real: 0.0058                   loss_D_fake: 0.0055 loss_G: 5.8881\n",
      "Epoch 6 [1000/3750] loss_D_real: 0.0052                   loss_D_fake: 0.0023 loss_G: 6.9156\n",
      "Epoch 6 [1100/3750] loss_D_real: 0.0658                   loss_D_fake: 0.1403 loss_G: 5.7929\n",
      "Epoch 6 [1200/3750] loss_D_real: 0.0031                   loss_D_fake: 0.0218 loss_G: 5.5029\n",
      "Epoch 6 [1300/3750] loss_D_real: 0.0332                   loss_D_fake: 0.0460 loss_G: 4.2144\n",
      "Epoch 6 [1400/3750] loss_D_real: 0.0462                   loss_D_fake: 0.0075 loss_G: 4.6721\n",
      "Epoch 6 [1500/3750] loss_D_real: 0.0007                   loss_D_fake: 0.0027 loss_G: 7.3944\n",
      "Epoch 6 [1600/3750] loss_D_real: 0.0008                   loss_D_fake: 0.0011 loss_G: 6.9807\n",
      "Epoch 6 [1700/3750] loss_D_real: 0.0586                   loss_D_fake: 0.1701 loss_G: 4.2083\n",
      "Epoch 6 [1800/3750] loss_D_real: 0.0004                   loss_D_fake: 0.1045 loss_G: 5.7221\n",
      "Epoch 6 [1900/3750] loss_D_real: 0.0020                   loss_D_fake: 0.0099 loss_G: 5.6875\n",
      "Epoch 6 [2000/3750] loss_D_real: 0.0054                   loss_D_fake: 0.0005 loss_G: 8.4846\n",
      "Epoch 6 [2100/3750] loss_D_real: 0.0055                   loss_D_fake: 0.0004 loss_G: 8.5179\n",
      "Epoch 6 [2200/3750] loss_D_real: 0.0023                   loss_D_fake: 0.1143 loss_G: 5.3910\n",
      "Epoch 6 [2300/3750] loss_D_real: 0.0275                   loss_D_fake: 0.0160 loss_G: 5.1828\n",
      "Epoch 6 [2400/3750] loss_D_real: 0.0007                   loss_D_fake: 0.0187 loss_G: 5.9380\n",
      "Epoch 6 [2500/3750] loss_D_real: 0.0087                   loss_D_fake: 0.0000 loss_G: 11.4162\n",
      "Epoch 6 [2600/3750] loss_D_real: 0.0825                   loss_D_fake: 0.3376 loss_G: 4.3538\n",
      "Epoch 6 [2700/3750] loss_D_real: 0.0064                   loss_D_fake: 0.2353 loss_G: 6.1837\n",
      "Epoch 6 [2800/3750] loss_D_real: 0.0057                   loss_D_fake: 0.0002 loss_G: 9.8045\n",
      "Epoch 6 [2900/3750] loss_D_real: 0.0032                   loss_D_fake: 0.0037 loss_G: 6.8756\n",
      "Epoch 6 [3000/3750] loss_D_real: 0.0006                   loss_D_fake: 0.0083 loss_G: 4.5926\n",
      "Epoch 6 [3100/3750] loss_D_real: 0.1369                   loss_D_fake: 0.0191 loss_G: 5.1979\n",
      "Epoch 6 [3200/3750] loss_D_real: 0.1628                   loss_D_fake: 0.0707 loss_G: 4.9812\n",
      "Epoch 6 [3300/3750] loss_D_real: 0.0149                   loss_D_fake: 0.0337 loss_G: 5.6351\n",
      "Epoch 6 [3400/3750] loss_D_real: 0.0071                   loss_D_fake: 0.0067 loss_G: 7.6381\n",
      "Epoch 6 [3500/3750] loss_D_real: 0.0023                   loss_D_fake: 0.0096 loss_G: 5.4799\n",
      "Epoch 6 [3600/3750] loss_D_real: 0.0003                   loss_D_fake: 0.0022 loss_G: 6.9959\n",
      "Epoch 6 [3700/3750] loss_D_real: 0.0006                   loss_D_fake: 0.0015 loss_G: 7.4208\n",
      "Epoch 7 [0/3750] loss_D_real: 0.2215                   loss_D_fake: 0.0073 loss_G: 5.6353\n",
      "Epoch 7 [100/3750] loss_D_real: 0.0002                   loss_D_fake: 0.6419 loss_G: 6.9190\n",
      "Epoch 7 [200/3750] loss_D_real: 0.0010                   loss_D_fake: 0.0292 loss_G: 5.3272\n",
      "Epoch 7 [300/3750] loss_D_real: 0.0136                   loss_D_fake: 0.0001 loss_G: 9.5280\n",
      "Epoch 7 [400/3750] loss_D_real: 0.0008                   loss_D_fake: 0.0003 loss_G: 8.5763\n",
      "Epoch 7 [500/3750] loss_D_real: 0.0021                   loss_D_fake: 0.0048 loss_G: 6.9404\n",
      "Epoch 7 [600/3750] loss_D_real: 0.0006                   loss_D_fake: 0.0000 loss_G: 46.3664\n",
      "Epoch 7 [700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 46.7571\n",
      "Epoch 7 [800/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.1497\n",
      "Epoch 7 [900/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 46.2157\n",
      "Epoch 7 [1000/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 46.0098\n",
      "Epoch 7 [1100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.6573\n",
      "Epoch 7 [1200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.5099\n",
      "Epoch 7 [1300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.5264\n",
      "Epoch 7 [1400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 46.1303\n",
      "Epoch 7 [1500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.6466\n",
      "Epoch 7 [1600/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 46.2833\n",
      "Epoch 7 [1700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.9424\n",
      "Epoch 7 [1800/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.5467\n",
      "Epoch 7 [1900/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.7709\n",
      "Epoch 7 [2000/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.5988\n",
      "Epoch 7 [2100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.7746\n",
      "Epoch 7 [2200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.3633\n",
      "Epoch 7 [2300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.5900\n",
      "Epoch 7 [2400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.4225\n",
      "Epoch 7 [2500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 46.4477\n",
      "Epoch 7 [2600/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 46.0331\n",
      "Epoch 7 [2700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.0780\n",
      "Epoch 7 [2800/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.4464\n",
      "Epoch 7 [2900/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.1263\n",
      "Epoch 7 [3000/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.1439\n",
      "Epoch 7 [3100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.9012\n",
      "Epoch 7 [3200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.3946\n",
      "Epoch 7 [3300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.7615\n",
      "Epoch 7 [3400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.9994\n",
      "Epoch 7 [3500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.6280\n",
      "Epoch 7 [3600/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.5486\n",
      "Epoch 7 [3700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.9854\n",
      "Epoch 8 [0/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.8143\n",
      "Epoch 8 [100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.7724\n",
      "Epoch 8 [200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.7923\n",
      "Epoch 8 [300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.9179\n",
      "Epoch 8 [400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.3101\n",
      "Epoch 8 [500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.0397\n",
      "Epoch 8 [600/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.5647\n",
      "Epoch 8 [700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.2602\n",
      "Epoch 8 [800/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.7415\n",
      "Epoch 8 [900/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.6840\n",
      "Epoch 8 [1000/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.0121\n",
      "Epoch 8 [1100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 43.9699\n",
      "Epoch 8 [1200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.6556\n",
      "Epoch 8 [1300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.4005\n",
      "Epoch 8 [1400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 45.0290\n",
      "Epoch 8 [1500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.5610\n",
      "Epoch 8 [1600/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.8665\n",
      "Epoch 8 [1700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 43.4586\n",
      "Epoch 8 [1800/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 43.1501\n",
      "Epoch 8 [1900/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 42.7719\n",
      "Epoch 8 [2000/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 44.7919\n",
      "Epoch 8 [2100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 42.4826\n",
      "Epoch 8 [2200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 40.6797\n",
      "Epoch 8 [2300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 38.3848\n",
      "Epoch 8 [2400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.6762\n",
      "Epoch 8 [2500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.7465\n",
      "Epoch 8 [2600/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.4051\n",
      "Epoch 8 [2700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.2106\n",
      "Epoch 8 [2800/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.5110\n",
      "Epoch 8 [2900/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.5344\n",
      "Epoch 8 [3000/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.6368\n",
      "Epoch 8 [3100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.4814\n",
      "Epoch 8 [3200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.6208\n",
      "Epoch 8 [3300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.2113\n",
      "Epoch 8 [3400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.8175\n",
      "Epoch 8 [3500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.1349\n",
      "Epoch 8 [3600/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.0213\n",
      "Epoch 8 [3700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.9463\n",
      "Epoch 9 [0/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.7835\n",
      "Epoch 9 [100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.5382\n",
      "Epoch 9 [200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.4204\n",
      "Epoch 9 [300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.6866\n",
      "Epoch 9 [400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.6327\n",
      "Epoch 9 [500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 51.7644\n",
      "Epoch 9 [600/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.6961\n",
      "Epoch 9 [700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.8720\n",
      "Epoch 9 [800/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.8909\n",
      "Epoch 9 [900/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.1511\n",
      "Epoch 9 [1000/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.9686\n",
      "Epoch 9 [1100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.8435\n",
      "Epoch 9 [1200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.5957\n",
      "Epoch 9 [1300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.6472\n",
      "Epoch 9 [1400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 51.9519\n",
      "Epoch 9 [1500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.7411\n",
      "Epoch 9 [1600/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.6188\n",
      "Epoch 9 [1700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.1811\n",
      "Epoch 9 [1800/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.7582\n",
      "Epoch 9 [1900/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.3222\n",
      "Epoch 9 [2000/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.9171\n",
      "Epoch 9 [2100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.4055\n",
      "Epoch 9 [2200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 51.9539\n",
      "Epoch 9 [2300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.1392\n",
      "Epoch 9 [2400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.0633\n",
      "Epoch 9 [2500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.5907\n",
      "Epoch 9 [2600/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.0924\n",
      "Epoch 9 [2700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.6289\n",
      "Epoch 9 [2800/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.4850\n",
      "Epoch 9 [2900/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.8493\n",
      "Epoch 9 [3000/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.7835\n",
      "Epoch 9 [3100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.6543\n",
      "Epoch 9 [3200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.4557\n",
      "Epoch 9 [3300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.1462\n",
      "Epoch 9 [3400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.8723\n",
      "Epoch 9 [3500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.4631\n",
      "Epoch 9 [3600/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 51.3089\n",
      "Epoch 9 [3700/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 53.3309\n",
      "Epoch 10 [0/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.6231\n",
      "Epoch 10 [100/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 51.8538\n",
      "Epoch 10 [200/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 51.4525\n",
      "Epoch 10 [300/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.2623\n",
      "Epoch 10 [400/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.7604\n",
      "Epoch 10 [500/3750] loss_D_real: 0.0000                   loss_D_fake: 0.0000 loss_G: 52.8479\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mh:\\PythonAILearn\\chapter8\\8_8_GAN_CUDA.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/PythonAILearn/chapter8/8_8_GAN_CUDA.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# 根据样本数据更新网络D\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/PythonAILearn/chapter8/8_8_GAN_CUDA.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m z_noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(x_real\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), Z_DIM, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/h%3A/PythonAILearn/chapter8/8_8_GAN_CUDA.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m x_fake \u001b[39m=\u001b[39m netG(z_noise)\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/PythonAILearn/chapter8/8_8_GAN_CUDA.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m y_fake \u001b[39m=\u001b[39m netD(x_fake)\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/PythonAILearn/chapter8/8_8_GAN_CUDA.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss_D_fake \u001b[39m=\u001b[39m loss_fn(y_fake, fake_label)\n",
      "File \u001b[1;32mc:\\Users\\cuish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cuish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mh:\\PythonAILearn\\chapter8\\8_8_GAN_CUDA.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/PythonAILearn/chapter8/8_8_GAN_CUDA.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/PythonAILearn/chapter8/8_8_GAN_CUDA.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# 输入：100@1*1\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/h%3A/PythonAILearn/chapter8/8_8_GAN_CUDA.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn1(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/PythonAILearn/chapter8/8_8_GAN_CUDA.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/PythonAILearn/chapter8/8_8_GAN_CUDA.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     X \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(X, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\cuish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cuish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cuish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    947\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    948\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[0;32m    949\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    950\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 952\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose2d(\n\u001b[0;32m    953\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[0;32m    954\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "viz_noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1, device=device)\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    for i, (x_real, _) in enumerate(train_loader):\n",
    "        x_real = x_real.to(device)\n",
    "        real_label = torch.full((x_real.size(0),), REAL_LABEL, device=device)\n",
    "        fake_label = torch.full((x_real.size(0),), FAKE_LABEL, device=device)\n",
    "\n",
    "        # 根据真实样本更新网络D\n",
    "        netD.zero_grad()\n",
    "        y_real = netD(x_real)\n",
    "        loss_D_real = loss_fn(y_real, real_label)\n",
    "        loss_D_real.backward()\n",
    "\n",
    "        # 根据样本数据更新网络D\n",
    "        z_noise = torch.randn(x_real.size(0), Z_DIM, 1, 1, device=device)\n",
    "        x_fake = netG(z_noise).detach()\n",
    "        y_fake = netD(x_fake)\n",
    "        loss_D_fake = loss_fn(y_fake, fake_label)\n",
    "        loss_D_fake.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # 根据样本数据更新网络G\n",
    "        netG.zero_grad()\n",
    "        x_fake = netG(z_noise)\n",
    "        y_fake_r = netD(x_fake)\n",
    "        loss_G = loss_fn(y_fake_r, real_label)\n",
    "        loss_G.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch {epoch} [{i}/{len(train_loader)}] loss_D_real: {loss_D_real.mean().item():.4f} loss_D_fake: {loss_D_fake.mean().item():.4f} loss_G: {loss_G.mean().item():.4f}')\n",
    "            \n",
    "            utils.save_image(x_real, os.path.join(OUT_PATH, f'real_samples_{epoch}.png'), normalize=True)\n",
    "            with torch.no_grad():\n",
    "                viz_sample = netG(viz_noise)\n",
    "                utils.save_image(viz_sample, os.path.join(OUT_PATH, f'fake_samples_{epoch}.png'), normalize=True)\n",
    "    torch.save(netG.state_dict(), os.path.join(OUT_PATH, f'netG_{epoch}.pth'))\n",
    "    torch.save(netD.state_dict(), os.path.join(OUT_PATH, f'netD_{epoch}.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
